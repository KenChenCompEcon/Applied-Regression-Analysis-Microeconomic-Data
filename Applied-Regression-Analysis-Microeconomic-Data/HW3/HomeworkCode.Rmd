---
title: "Homework 3"
author: "Group 3"
date: "November 27, 2018"
output: pdf_document
---
# Homework 3
### Group 3: Lindsay Liebert and Ken Chen

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(caret)
library(survey)
```

### import data and cleaning
```{r}
data = read.csv("usa_00002.csv")
library(dplyr)
data = filter(data, RACE==1 & HISPAN==0)
# education
data$edu = (data$EDUCD<30)*1 + (data$EDUCD %in% c(30,40,50,60,61))*2 + (data$EDUCD %in% c(62,64))*3 + (data$EDUCD==63)*4 + 
  (data$EDUCD %in% c(65,70,71))*5 + (data$EDUCD %in% c(81,82,83))*6 + (data$EDUCD==101)*7 + (data$EDUCD==114)*8 +
  (data$EDUCD==115)*9 + (data$EDUCD==116)*10
data$edu = as.factor(data$edu)
# whether has a child
data$has_child = as.factor((data$NCHILD>0)*1)
# whether live in msa
data$msa = as.factor((data$MET2013!=0)*1)
```

### Calculate propensity score
```{r}
design = svydesign(ids=~1, weights=~PERWT, data=data)

logit_order2 = svyglm(has_child ~ AGE + I(AGE^2) + edu + msa +
                        AGE:edu + AGE:msa + edu:msa, family = binomial(link = "logit"), design = design)

data$ps = fitted.values(logit_order2)
hist(data$ps, main="Predicted Values of Second Approx. Logit Model", 
     xlab="Predicted Values", col="lightcyan")
attach(data)
```

### Q1
```{r}
# Model training for the untreated
set.seed(47)
data_untreated = data[data$has_child==0,]
mse.untreated = matrix(NA, nrow=3, ncol=10)
colnames(mse.untreated) = 1:10
rownames(mse.untreated) = c('UHRSWORK', 'INCEARN', 'INCWAGE')
idx = sample(1:5, nrow(data_untreated), replace = TRUE)
fold.weights.untreated = rep(NA, 5)
for (i in 1:5){
 fold.weights.untreated[i] = sum(data_untreated[idx==i,'PERWT'])
}
fold.mean.untreated = function(x){
  return(weighted.mean(x, fold.weights.untreated))
}
for (i in 1:10){
  mse_mat = matrix(NA, nrow=3, ncol=5)
  # 5-fold cross-validation
  for (j in 1:5){
    # split the data into training and testing data
    train.set = data_untreated[idx!=j,]
    test.set = data_untreated[idx==j,]
    # UHRSWORK
    model1 = lm(UHRSWORK ~ poly(ps, degree=i), data=train.set, weights=PERWT) # regression model with weights
    pred1 = predict(model1, test.set)
    error1 = weighted.mean((test.set$UHRSWORK-pred1)^2, w=test.set$PERWT) # calculate MSE with weights for each fold
    # INCEARN
    model2 = lm(INCEARN ~ poly(ps, degree=i), data=train.set, weights=PERWT) # regression model with weights
    pred2 = predict(model2, test.set)
    error2 = weighted.mean((test.set$INCEARN-pred2)^2, w=test.set$PERWT) # calculate MSE with weights for each fold
    # INCWAGE
    model3 = lm(INCWAGE ~ poly(ps, degree=i), data=train.set, weights=PERWT) # regression model with weights
    pred3 = predict(model3, test.set)
    error3 = weighted.mean((test.set$INCWAGE-pred3)^2, w=test.set$PERWT) # calculate MSE with weights for each fold
    
    mse_mat[1,j] = error1
    mse_mat[2,j] = error2
    mse_mat[3,j] = error3
  }
  mse.untreated[,i] = apply(mse_mat, 1, fold.mean.untreated) # calculate the total MSE with weights
}
```

```{r}
plot(1:10, mse.untreated[1,], main="Total MSE for defferent J: UHRSWORK", type="l", col="blue", xlab = "polynomial number", ylab = "MSE_UHRSWORK")
```

```{r}
plot(1:10, mse.untreated[2,], main="Total MSE for defferent J: INCEARN", type="l", col="blue", xlab = "polynomial number", ylab = "MSE_INCEARN")
```

```{r}
plot(1:10, mse.untreated[3,], main="Total MSE for defferent J: INCWAGE", type="l", col="blue", xlab = "polynomial number", ylab = "MSE_INCWAGE")
```



```{r}
# Model training for the treated: test from J=1 to J=10
data_treated = data[data$has_child==1,]
mse.treated = matrix(NA, nrow=3, ncol=10)
colnames(mse.treated) = 1:10
rownames(mse.treated) = c('UHRSWORK', 'INCEARN', 'INCWAGE')
idx = sample(1:5, nrow(data_treated), replace = TRUE)
fold.weights.treated = rep(NA, 5)
for (i in 1:5){
 fold.weights.treated[i] = sum(data_treated[idx==i,'PERWT'])
}
fold.mean.treated = function(x){
  return(weighted.mean(x, fold.weights.treated))
}
for (i in 1:10){
  mse_mat = matrix(NA, nrow=3, ncol=5)
  # 5-fold cross validation
  for (j in 1:5){
    # split the data into training and testing data
    train.set = data_treated[idx!=j,]
    test.set = data_treated[idx==j,]
    # UHRSWORK
    model1 = lm(UHRSWORK ~ poly(ps, degree=i), data=train.set, weights=PERWT)
    pred1 = predict(model1, test.set)
    error1 = weighted.mean((test.set$UHRSWORK-pred1)^2, w=test.set$PERWT)
    # INCEARN
    model2 = lm(INCEARN ~ poly(ps, degree=i), data=train.set, weights=PERWT)
    pred2 = predict(model2, test.set)
    error2 = weighted.mean((test.set$INCEARN-pred2)^2, w=test.set$PERWT)
    # INCWAGE
    model3 = lm(INCWAGE ~ poly(ps, degree=i), data=train.set, weights=PERWT)
    pred3 = predict(model3, test.set)
    error3 = weighted.mean((test.set$INCWAGE-pred3)^2, w=test.set$PERWT)
    
    mse_mat[1,j] = error1
    mse_mat[2,j] = error2
    mse_mat[3,j] = error3
  }
  mse.treated[,i] = apply(mse_mat, 1, fold.mean.treated) # calculate the total MSE with weights
}
```

```{r}
plot(1:10, mse.treated[1,], main="Total MSE for defferent J: UHRSWORK", type="l", col="blue", xlab = "polynomial number", ylab = "MSE_UHRSWORK")
```

```{r}
plot(1:10, mse.treated[2,], main="Total MSE for defferent J: INCEARN", type="l", col="blue", xlab = "polynomial number", ylab = "MSE_INCEARN")
```

```{r}
plot(1:10, mse.treated[3,], main="Total MSE for defferent J: INCWAGE", type="l", col="blue", xlab = "polynomial number", ylab = "MSE_INCWAGE")
```

```{r}
best.idx.untreated = apply(mse.untreated, 1, which.min)
best.idx.treated = apply(mse.treated, 1, which.min)
rbind(best.idx.treated, best.idx.untreated)
```
As is reported above, the optimal J for the treated are 3, 6, 5 on HRSWORK, INCEARN and INCWAGE respectively; and 4, 3, 3 for the untreated. We pick the best J-degrees for both the treated and the untreated, on the three dependent variables. Then we plug these optimal smoothing parameters into the following models and estimate the att's.

```{r}
att.1 = rep(NA, 3); names(att.1) = c('UHRSWORK','INCEARN', 'INCWAGE')

# compute att for UHRSWORK
model1 = lm(UHRSWORK ~ poly(ps, degree=best.idx.treated[1]), data=data_treated, weights=PERWT)
model0 = lm(UHRSWORK ~ poly(ps, degree=best.idx.untreated[1]), data=data_untreated, weights=PERWT)
att = weighted.mean((fitted.values(model1)-predict(model0, data_treated)), data_treated$PERWT)
att.1[1] = att
# compute att for INCEARN
model1 = lm(INCEARN ~ poly(ps, degree=best.idx.treated[2]), data=data_treated, weights=PERWT)
model0 = lm(INCEARN ~ poly(ps, degree=best.idx.untreated[2]), data=data_untreated, weights=PERWT)
att = weighted.mean((fitted.values(model1)-predict(model0, data_treated)), data_treated$PERWT)
att.1[2] = att
# compute att for INCWAGE
model1 = lm(INCWAGE ~ poly(ps, degree=best.idx.treated[3]), data=data_treated, weights=PERWT)
model0 = lm(INCWAGE ~ poly(ps, degree=best.idx.untreated[3]), data=data_untreated, weights=PERWT)
att = weighted.mean((fitted.values(model1)-predict(model0, data_treated)), data_treated$PERWT)
att.1[3] = att

as.data.frame(att.1)
```
Above are the estimated ATT's using series estimation, on three different dependent variables.

### Q2
Use a k-nearest neighbor estimator to asses the impact of treatment on the treated for each of the dependent variables.
```{r}
# Here we take advantage of the knn function from 'caret', which can report the arithmetic mean of the k nearest neighbors from trainset that are matched to the testset.

# define the knn fucntion that returns the predicted values for the test data
knn_ck = function(k, train.set, test.set, key){
  x = as.data.frame(train.set$ps); y = as.numeric(train.set[,key])
  model = knnreg(x, y, k)
  return(predict(model, test.set$ps))
}

# 5-fold cross validation for the untreated: we test from k=2 to k=40
set.seed(234)
mse.untreated.knn = matrix(NA, nrow=3, ncol=39)
colnames(mse.untreated.knn) = c(2:40)
rownames(mse.untreated.knn) = c('UHRSWORK', 'INCEARN', 'INCWAGE')
idx = sample(1:5, size=nrow(data_untreated), replace=TRUE)
fold.weights.untreated = rep(NA, 5)
for (i in 1:5){
 fold.weights.untreated[i] = sum(data_untreated[idx==i,'PERWT'])
}
fold.mean.untreated = function(x){
  return(weighted.mean(x, fold.weights.untreated))
}
for (i in 2:40){
  mse_mat = matrix(NA, nrow=3, ncol=5)
  for (j in 1:5){
    train.set = data_untreated[idx!=j,]
    test.set = data_untreated[idx==j,]
    mse_mat[1,j] = weighted.mean((knn_ck(i, train.set, test.set, 'UHRSWORK')-test.set$UHRSWORK)^2, test.set$PERWT)
    mse_mat[2,j] = weighted.mean((knn_ck(i, train.set, test.set, 'INCEARN')-test.set$UHRSWORK)^2, test.set$PERWT)
    mse_mat[3,j] = weighted.mean((knn_ck(i, train.set, test.set, 'INCWAGE')-test.set$UHRSWORK)^2, test.set$PERWT)
  }
  mse.untreated.knn[,i-1] = apply(mse_mat, 1, fold.mean.untreated) # calculate the total MSE with weights
}
```

```{r}
plot(2:40, mse.untreated.knn[1,], main="Total MSE for defferent K: UHRSWORK
     ", type="l", col="blue", xlab = "polynomial number", ylab = "MSE_UHRSWORK")
```

```{r}
plot(2:40, mse.untreated.knn[2,], main = "Total MSE for defferent K: INCEARN", 
     type="l", col="blue", xlab = "polynomial number", ylab = "MSE_INCEARN")
```

```{r}
plot(2:40, mse.untreated.knn[3,], main = "Total MSE for defferent K: INCWAGE", 
     type="l", col="blue", xlab = "polynomial number", ylab = "MSE_INCEARN")
```

```{r}
# cross validation for the treated
mse.treated.knn = matrix(NA, nrow=3, ncol=39)
colnames(mse.treated.knn) = c(2:40)
rownames(mse.treated.knn) = c('UHRSWORK', 'INCEARN', 'INCWAGE')
idx = sample(1:5, size=nrow(data_treated), replace=TRUE)
fold.weights.treated = rep(NA, 5)
for (i in 1:5){
 fold.weights.treated[i] = sum(data_treated[idx==i,'PERWT'])
}
fold.mean.treated = function(x){
  return(weighted.mean(x, fold.weights.treated))
}
for (i in 2:40){
  mse_mat = matrix(NA, nrow=3, ncol=5)
  for (j in 1:5){
    train.set = data_treated[idx!=j,]
    test.set = data_treated[idx==j,]
    mse_mat[1,j] = weighted.mean((knn_ck(i, train.set, test.set, 'UHRSWORK')-test.set$UHRSWORK)^2, test.set$PERWT)
    mse_mat[2,j] = weighted.mean((knn_ck(i, train.set, test.set, 'INCEARN')-test.set$UHRSWORK)^2, test.set$PERWT)
    mse_mat[3,j] = weighted.mean((knn_ck(i, train.set, test.set, 'INCWAGE')-test.set$UHRSWORK)^2, test.set$PERWT)
  }
  mse.treated.knn[,i-1] = apply(mse_mat, 1, fold.mean.treated)  # calculate the total MSE with weights
}
```

```{r}
plot(2:40, mse.treated.knn[1,], main="Total MSE for defferent K: UHRSWORK
     ", type="l", col="blue", xlab = "polynomial number", ylab = "MSE_UHRSWORK")
```
```{r}
plot(2:40, mse.treated.knn[2,], main="Total MSE for defferent K: INCEARN
     ", type="l", col="blue", xlab = "polynomial number", ylab = "MSE_UHRSWORK")
```
```{r}
plot(2:40, mse.treated.knn[3,], main="Total MSE for defferent K: INCWAGE
     ", type="l", col="blue", xlab = "polynomial number", ylab = "MSE_UHRSWORK")
```

```{r}
best.idx.untreated.knn = apply(mse.untreated.knn, 1, which.min) + 1 
best.idx.treated.knn = apply(mse.treated.knn, 1, which.min) + 1

best.idx.untreated.knn
best.idx.treated.knn
```
As reported above, the optimal K for UHRSWORK is 3 on the untreated and 23 on the treated; 36 for INCEARN on the untreated and 
32 on the treated; 36 for INCWAGE on the untreated and 34 on the treated. Let's compute the ATT's under these K's.

```{r}
# Calculate att with K=11
att.2 = rep(NA, 3)
i = 1
for (outcome in c('UHRSWORK','INCEARN','INCWAGE')){
  model.0 = knnreg(as.data.frame(data_untreated$ps), data_untreated[,outcome], best.idx.untreated.knn[i])
  pred.0 = predict(model.0, data_treated$ps)
  model.1 = knnreg(as.data.frame(data_treated$ps), data_treated[,outcome], 20)
  pred.1 = predict(model.1, data_treated$ps)
  att.2[i] = weighted.mean((pred.1-pred.0), data_treated$PERWT)
  i = i+1
}
names(att.2) = c('UHRSWORK','INCEARN', 'INCWAGE')
as.data.frame(att.2)
```

Here, we take K as 11 for both the KNN model on the untreated and the treated.
```{r}
# Calculate att with K=11
att.2 = rep(NA, 3)
i = 1
for (outcome in c('UHRSWORK','INCEARN','INCWAGE')){
  model.0 = knnreg(as.data.frame(data_untreated$ps), data_untreated[,outcome], 11)
  pred.0 = predict(model.0, data_treated$ps)
  model.1 = knnreg(as.data.frame(data_treated$ps), data_treated[,outcome], 11)
  pred.1 = predict(model.1, data_treated$ps)
  att.2[i] = weighted.mean((pred.1-pred.0), data_treated$PERWT)
  i = i+1
}
names(att.2) = c('UHRSWORK','INCEARN', 'INCWAGE')
as.data.frame(att.2)
```
Above is the KNN estimates of ATT, on three different dependent variables.

### Q3: Use an IPW estimator to asses the impact of treatment on the treated for each of the dependent variables.
```{r}
# compute the inverse probability weights
data$ipw_att = (has_child==1)*PERWT + (has_child==0)*PERWT*ps/(1-ps)

# short regressions
att.3.short = rep(NA, 3)
att.3.short[1] = summary(lm(UHRSWORK ~ has_child, data = data, weights = ipw_att))$coefficients[2,1]
att.3.short[2] = summary(lm(INCEARN ~ has_child, data = data, weights = ipw_att))$coefficients[2,1]
att.3.short[3] = summary(lm(INCWAGE ~ has_child, data = data, weights = ipw_att))$coefficients[2,1]

# long regressions
att.3.long = rep(NA, 3)
att.3.long[1] = summary(lm(UHRSWORK ~ has_child + AGE + I(AGE^2) + edu + msa +
                        AGE:edu + AGE:msa + edu:msa, data = data, weights = ipw_att))$coefficients[2,1]
att.3.long[2] = summary(lm(INCEARN ~ has_child + AGE + I(AGE^2) + edu + msa +
                        AGE:edu + AGE:msa + edu:msa, data = data, weights = ipw_att))$coefficients[2,1]
att.3.long[3] = summary(lm(INCWAGE ~ has_child + AGE + I(AGE^2) + edu + msa +
                        AGE:edu + AGE:msa + edu:msa, data = data, weights = ipw_att))$coefficients[2,1]

names(att.3.short) = c('UHRSWORK','INCEARN', 'INCWAGE')
names(att.3.long) = c('UHRSWORK','INCEARN', 'INCWAGE')

att.3 = rbind(att.3.short, att.3.long)
rownames(att.3) = c('short', 'long')
att.3
```
Above we report the estimated ATT of having child on women's usual hours of work, INCEARN and INCWAGE. We tried both long and short, and they turn out to be quite close. Let's look at the short regressions. For those who are already mothers, having a child is going to reduce their usual weekly working hours by 7.1, reduce their INCEARN by 7716.9, and reduce their INCWAGE by 7683.8.

### Q4: Compare and contrast the three estimates of the ATT.
```{r}
att = cbind(att.1, att.2, att.3.short, att.3.long)
rownames(att) = c('UHRSWORK','INCEARN','INCWAGE')
colnames(att) = c('J-degree','KNN','IPW.S','IPW.L')
att
```
Here we report the estimates of ATT, using non-parametric models such as Series Estimator and K-nearest-neighbor Estimator; for parametric estimates, we present IPW estimates using both short and long regression forms. All of the estimates are only slightly different, except the KNN ones. But they are still on the same magnitude, and the deviation from others is probably dur to the our not using optimal smoothing parameters.

### Q5: Use an IPW estimator to asses the impact of treatment on the untreated and the impact of treatment on population. Compare and contrast the ATT, ATN, and the ATE.
```{r}
atn.ate = matrix(NA, 3, 4)
rownames(atn.ate) = c('UHRSWORK','INCEARN', 'INCWAGE')
colnames(atn.ate) = c('ATN.S', 'ATN.L','ATE.S','ATE.L')
# Calculate ATN
data$ipw_atn = (has_child==1)*PERWT*(1-ps)/ps  + (has_child==0)*PERWT
atn.ate[1,1] = summary(lm(UHRSWORK ~ has_child, data = data, weights = ipw_atn))$coefficients[2,1]
atn.ate[1,2] = summary(lm(UHRSWORK ~ has_child + AGE + I(AGE^2) + edu + msa +
                        AGE:edu + AGE:msa + edu:msa, data = data, weights = ipw_atn))$coefficients[2,1]
atn.ate[2,1] = summary(lm(INCEARN ~ has_child, data = data, weights = ipw_atn))$coefficients[2,1]
atn.ate[2,2] = summary(lm(INCEARN ~ has_child + AGE + I(AGE^2) + edu + msa +
                        AGE:edu + AGE:msa + edu:msa, data = data, weights = ipw_atn))$coefficients[2,1]
atn.ate[3,1] = summary(lm(INCWAGE ~ has_child, data = data, weights = ipw_atn))$coefficients[2,1]
atn.ate[3,2] = summary(lm(INCWAGE ~ has_child + AGE + I(AGE^2) + edu + msa +
                        AGE:edu + AGE:msa + edu:msa, data = data, weights = ipw_atn))$coefficients[2,1]

# Calculate ATE
data$ipw_ate = (has_child==1)*PERWT/ps  + (has_child==0)*PERWT/(1-ps)
atn.ate[1,3] = summary(lm(UHRSWORK ~ has_child, data = data, weights = ipw_ate))$coefficients[2,1]
atn.ate[1,4] = summary(lm(UHRSWORK ~ has_child + AGE + I(AGE^2) + edu + msa +
                        AGE:edu + AGE:msa + edu:msa, data = data, weights = ipw_ate))$coefficients[2,1]
atn.ate[2,3] = summary(lm(INCEARN ~ has_child, data = data, weights = ipw_ate))$coefficients[2,1]
atn.ate[2,4] = summary(lm(INCEARN ~ has_child + AGE + I(AGE^2) + edu + msa +
                        AGE:edu + AGE:msa + edu:msa, data = data, weights = ipw_ate))$coefficients[2,1]
atn.ate[3,3] = summary(lm(INCWAGE ~ has_child, data = data, weights = ipw_ate))$coefficients[2,1]
atn.ate[3,4] = summary(lm(INCWAGE ~ has_child + AGE + I(AGE^2) + edu + msa +
                        AGE:edu + AGE:msa + edu:msa, data = data, weights = ipw_ate))$coefficients[2,1]

att.5 = att[, c('IPW.S','IPW.L')]
colnames(att.5) = c('ATT.S', 'ATT.L')
cbind(att.5,atn.ate)
```
From the table you can see that in each category the ATN is the largest effect and the ATT is the smallest effect. This implies that those who do not have children would experience a bigger reduction in wages, hours, earnings if they were to have a child. For those with children, they are also experience reduced hours, wage, earnings but to a lesser degree than for those that do not have children.
